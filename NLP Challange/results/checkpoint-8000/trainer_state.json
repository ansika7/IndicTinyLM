{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.63768115942029,
  "eval_steps": 500,
  "global_step": 8000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.057971014492753624,
      "grad_norm": 1.604691982269287,
      "learning_rate": 0.0004942608695652174,
      "loss": 5.8729,
      "step": 100
    },
    {
      "epoch": 0.11594202898550725,
      "grad_norm": 1.6716281175613403,
      "learning_rate": 0.000488463768115942,
      "loss": 5.773,
      "step": 200
    },
    {
      "epoch": 0.17391304347826086,
      "grad_norm": 1.5181828737258911,
      "learning_rate": 0.00048266666666666667,
      "loss": 5.7716,
      "step": 300
    },
    {
      "epoch": 0.2318840579710145,
      "grad_norm": 1.657565951347351,
      "learning_rate": 0.00047686956521739133,
      "loss": 5.758,
      "step": 400
    },
    {
      "epoch": 0.2898550724637681,
      "grad_norm": 1.6968783140182495,
      "learning_rate": 0.000471072463768116,
      "loss": 5.7494,
      "step": 500
    },
    {
      "epoch": 0.34782608695652173,
      "grad_norm": 1.6718605756759644,
      "learning_rate": 0.0004652753623188406,
      "loss": 5.7347,
      "step": 600
    },
    {
      "epoch": 0.4057971014492754,
      "grad_norm": 1.4573394060134888,
      "learning_rate": 0.00045947826086956524,
      "loss": 5.7402,
      "step": 700
    },
    {
      "epoch": 0.463768115942029,
      "grad_norm": 1.4788851737976074,
      "learning_rate": 0.00045368115942028984,
      "loss": 5.758,
      "step": 800
    },
    {
      "epoch": 0.5217391304347826,
      "grad_norm": 1.4374886751174927,
      "learning_rate": 0.0004478840579710145,
      "loss": 5.7103,
      "step": 900
    },
    {
      "epoch": 0.5797101449275363,
      "grad_norm": 1.5531623363494873,
      "learning_rate": 0.00044208695652173916,
      "loss": 5.7199,
      "step": 1000
    },
    {
      "epoch": 0.6376811594202898,
      "grad_norm": 1.7600126266479492,
      "learning_rate": 0.00043628985507246376,
      "loss": 5.736,
      "step": 1100
    },
    {
      "epoch": 0.6956521739130435,
      "grad_norm": 1.6260586977005005,
      "learning_rate": 0.0004304927536231884,
      "loss": 5.736,
      "step": 1200
    },
    {
      "epoch": 0.7536231884057971,
      "grad_norm": 1.1634711027145386,
      "learning_rate": 0.000424695652173913,
      "loss": 5.7524,
      "step": 1300
    },
    {
      "epoch": 0.8115942028985508,
      "grad_norm": 1.5203295946121216,
      "learning_rate": 0.0004188985507246377,
      "loss": 5.7056,
      "step": 1400
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 1.2856703996658325,
      "learning_rate": 0.00041310144927536233,
      "loss": 5.7282,
      "step": 1500
    },
    {
      "epoch": 0.927536231884058,
      "grad_norm": 1.3812986612319946,
      "learning_rate": 0.00040730434782608693,
      "loss": 5.7277,
      "step": 1600
    },
    {
      "epoch": 0.9855072463768116,
      "grad_norm": 1.4042267799377441,
      "learning_rate": 0.00040150724637681164,
      "loss": 5.6996,
      "step": 1700
    },
    {
      "epoch": 1.0434782608695652,
      "grad_norm": 1.3225194215774536,
      "learning_rate": 0.00039571014492753624,
      "loss": 5.6948,
      "step": 1800
    },
    {
      "epoch": 1.1014492753623188,
      "grad_norm": 1.8332465887069702,
      "learning_rate": 0.0003899130434782609,
      "loss": 5.7173,
      "step": 1900
    },
    {
      "epoch": 1.1594202898550725,
      "grad_norm": 1.2987161874771118,
      "learning_rate": 0.00038411594202898556,
      "loss": 5.704,
      "step": 2000
    },
    {
      "epoch": 1.2173913043478262,
      "grad_norm": 1.426327109336853,
      "learning_rate": 0.00037831884057971016,
      "loss": 5.7122,
      "step": 2100
    },
    {
      "epoch": 1.2753623188405796,
      "grad_norm": 1.5092190504074097,
      "learning_rate": 0.0003725217391304348,
      "loss": 5.7103,
      "step": 2200
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 1.3577637672424316,
      "learning_rate": 0.0003667246376811594,
      "loss": 5.6984,
      "step": 2300
    },
    {
      "epoch": 1.391304347826087,
      "grad_norm": 1.4766803979873657,
      "learning_rate": 0.0003609275362318841,
      "loss": 5.6998,
      "step": 2400
    },
    {
      "epoch": 1.4492753623188406,
      "grad_norm": 1.2458980083465576,
      "learning_rate": 0.0003551304347826087,
      "loss": 5.7148,
      "step": 2500
    },
    {
      "epoch": 1.5072463768115942,
      "grad_norm": 1.3383302688598633,
      "learning_rate": 0.00034933333333333333,
      "loss": 5.684,
      "step": 2600
    },
    {
      "epoch": 1.5652173913043477,
      "grad_norm": 1.315062165260315,
      "learning_rate": 0.000343536231884058,
      "loss": 5.6929,
      "step": 2700
    },
    {
      "epoch": 1.6231884057971016,
      "grad_norm": 1.394887924194336,
      "learning_rate": 0.0003377391304347826,
      "loss": 5.6957,
      "step": 2800
    },
    {
      "epoch": 1.681159420289855,
      "grad_norm": 1.3358594179153442,
      "learning_rate": 0.00033194202898550725,
      "loss": 5.7081,
      "step": 2900
    },
    {
      "epoch": 1.7391304347826086,
      "grad_norm": 1.3734347820281982,
      "learning_rate": 0.00032614492753623185,
      "loss": 5.704,
      "step": 3000
    },
    {
      "epoch": 1.7971014492753623,
      "grad_norm": 1.3106489181518555,
      "learning_rate": 0.0003203478260869565,
      "loss": 5.7092,
      "step": 3100
    },
    {
      "epoch": 1.855072463768116,
      "grad_norm": 1.2544612884521484,
      "learning_rate": 0.00031455072463768116,
      "loss": 5.722,
      "step": 3200
    },
    {
      "epoch": 1.9130434782608696,
      "grad_norm": 1.2825136184692383,
      "learning_rate": 0.0003087536231884058,
      "loss": 5.6786,
      "step": 3300
    },
    {
      "epoch": 1.971014492753623,
      "grad_norm": 1.4993767738342285,
      "learning_rate": 0.0003029565217391305,
      "loss": 5.7028,
      "step": 3400
    },
    {
      "epoch": 2.028985507246377,
      "grad_norm": 1.5487370491027832,
      "learning_rate": 0.0002971594202898551,
      "loss": 5.6777,
      "step": 3500
    },
    {
      "epoch": 2.0869565217391304,
      "grad_norm": 1.5382572412490845,
      "learning_rate": 0.00029136231884057973,
      "loss": 5.6781,
      "step": 3600
    },
    {
      "epoch": 2.1449275362318843,
      "grad_norm": 1.2951405048370361,
      "learning_rate": 0.0002855652173913044,
      "loss": 5.6863,
      "step": 3700
    },
    {
      "epoch": 2.2028985507246377,
      "grad_norm": 1.2390425205230713,
      "learning_rate": 0.000279768115942029,
      "loss": 5.6776,
      "step": 3800
    },
    {
      "epoch": 2.260869565217391,
      "grad_norm": 1.237635850906372,
      "learning_rate": 0.00027397101449275365,
      "loss": 5.684,
      "step": 3900
    },
    {
      "epoch": 2.318840579710145,
      "grad_norm": 1.359555721282959,
      "learning_rate": 0.00026817391304347825,
      "loss": 5.7097,
      "step": 4000
    },
    {
      "epoch": 2.3768115942028984,
      "grad_norm": 1.3931331634521484,
      "learning_rate": 0.0002623768115942029,
      "loss": 5.6955,
      "step": 4100
    },
    {
      "epoch": 2.4347826086956523,
      "grad_norm": 1.4392648935317993,
      "learning_rate": 0.00025657971014492756,
      "loss": 5.6583,
      "step": 4200
    },
    {
      "epoch": 2.4927536231884058,
      "grad_norm": 1.4823716878890991,
      "learning_rate": 0.00025078260869565216,
      "loss": 5.6865,
      "step": 4300
    },
    {
      "epoch": 2.550724637681159,
      "grad_norm": 1.6178542375564575,
      "learning_rate": 0.0002449855072463768,
      "loss": 5.6957,
      "step": 4400
    },
    {
      "epoch": 2.608695652173913,
      "grad_norm": 1.5840052366256714,
      "learning_rate": 0.00023918840579710148,
      "loss": 5.6556,
      "step": 4500
    },
    {
      "epoch": 2.6666666666666665,
      "grad_norm": 1.5521167516708374,
      "learning_rate": 0.0002333913043478261,
      "loss": 5.6872,
      "step": 4600
    },
    {
      "epoch": 2.7246376811594204,
      "grad_norm": 1.2752974033355713,
      "learning_rate": 0.00022759420289855073,
      "loss": 5.6729,
      "step": 4700
    },
    {
      "epoch": 2.782608695652174,
      "grad_norm": 1.4734410047531128,
      "learning_rate": 0.00022179710144927536,
      "loss": 5.6618,
      "step": 4800
    },
    {
      "epoch": 2.8405797101449277,
      "grad_norm": 1.292382836341858,
      "learning_rate": 0.000216,
      "loss": 5.6781,
      "step": 4900
    },
    {
      "epoch": 2.898550724637681,
      "grad_norm": 1.2769322395324707,
      "learning_rate": 0.00021020289855072465,
      "loss": 5.6713,
      "step": 5000
    },
    {
      "epoch": 2.9565217391304346,
      "grad_norm": 1.3075593709945679,
      "learning_rate": 0.00020440579710144928,
      "loss": 5.6719,
      "step": 5100
    },
    {
      "epoch": 3.0144927536231885,
      "grad_norm": 1.2370485067367554,
      "learning_rate": 0.0001986086956521739,
      "loss": 5.6545,
      "step": 5200
    },
    {
      "epoch": 3.072463768115942,
      "grad_norm": 1.371703028678894,
      "learning_rate": 0.00019281159420289856,
      "loss": 5.65,
      "step": 5300
    },
    {
      "epoch": 3.130434782608696,
      "grad_norm": 1.230385661125183,
      "learning_rate": 0.0001870144927536232,
      "loss": 5.6461,
      "step": 5400
    },
    {
      "epoch": 3.1884057971014492,
      "grad_norm": 1.4756999015808105,
      "learning_rate": 0.00018121739130434782,
      "loss": 5.6507,
      "step": 5500
    },
    {
      "epoch": 3.246376811594203,
      "grad_norm": 1.2341750860214233,
      "learning_rate": 0.00017542028985507248,
      "loss": 5.6547,
      "step": 5600
    },
    {
      "epoch": 3.3043478260869565,
      "grad_norm": 1.201129674911499,
      "learning_rate": 0.0001696231884057971,
      "loss": 5.6515,
      "step": 5700
    },
    {
      "epoch": 3.36231884057971,
      "grad_norm": 1.4939030408859253,
      "learning_rate": 0.00016382608695652174,
      "loss": 5.6305,
      "step": 5800
    },
    {
      "epoch": 3.420289855072464,
      "grad_norm": 1.4174376726150513,
      "learning_rate": 0.00015802898550724636,
      "loss": 5.6589,
      "step": 5900
    },
    {
      "epoch": 3.4782608695652173,
      "grad_norm": 1.3823999166488647,
      "learning_rate": 0.000152231884057971,
      "loss": 5.6512,
      "step": 6000
    },
    {
      "epoch": 3.536231884057971,
      "grad_norm": 1.4966282844543457,
      "learning_rate": 0.00014643478260869568,
      "loss": 5.6873,
      "step": 6100
    },
    {
      "epoch": 3.5942028985507246,
      "grad_norm": 1.4873498678207397,
      "learning_rate": 0.0001406376811594203,
      "loss": 5.6348,
      "step": 6200
    },
    {
      "epoch": 3.6521739130434785,
      "grad_norm": 1.4071396589279175,
      "learning_rate": 0.00013484057971014494,
      "loss": 5.6663,
      "step": 6300
    },
    {
      "epoch": 3.710144927536232,
      "grad_norm": 1.3874605894088745,
      "learning_rate": 0.00012904347826086956,
      "loss": 5.6427,
      "step": 6400
    },
    {
      "epoch": 3.7681159420289854,
      "grad_norm": 1.4628057479858398,
      "learning_rate": 0.0001232463768115942,
      "loss": 5.6709,
      "step": 6500
    },
    {
      "epoch": 3.8260869565217392,
      "grad_norm": 1.5169169902801514,
      "learning_rate": 0.00011744927536231885,
      "loss": 5.6531,
      "step": 6600
    },
    {
      "epoch": 3.8840579710144927,
      "grad_norm": 1.464477300643921,
      "learning_rate": 0.00011165217391304348,
      "loss": 5.6619,
      "step": 6700
    },
    {
      "epoch": 3.942028985507246,
      "grad_norm": 1.2669657468795776,
      "learning_rate": 0.00010585507246376812,
      "loss": 5.6622,
      "step": 6800
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.166953444480896,
      "learning_rate": 0.00010005797101449275,
      "loss": 5.6328,
      "step": 6900
    },
    {
      "epoch": 4.057971014492754,
      "grad_norm": 1.591470718383789,
      "learning_rate": 9.42608695652174e-05,
      "loss": 5.6356,
      "step": 7000
    },
    {
      "epoch": 4.115942028985507,
      "grad_norm": 1.4128962755203247,
      "learning_rate": 8.846376811594204e-05,
      "loss": 5.654,
      "step": 7100
    },
    {
      "epoch": 4.173913043478261,
      "grad_norm": 1.4496184587478638,
      "learning_rate": 8.266666666666667e-05,
      "loss": 5.61,
      "step": 7200
    },
    {
      "epoch": 4.231884057971015,
      "grad_norm": 1.2002143859863281,
      "learning_rate": 7.686956521739131e-05,
      "loss": 5.6369,
      "step": 7300
    },
    {
      "epoch": 4.2898550724637685,
      "grad_norm": 1.313265323638916,
      "learning_rate": 7.107246376811595e-05,
      "loss": 5.6264,
      "step": 7400
    },
    {
      "epoch": 4.3478260869565215,
      "grad_norm": 1.2899450063705444,
      "learning_rate": 6.527536231884058e-05,
      "loss": 5.6287,
      "step": 7500
    },
    {
      "epoch": 4.405797101449275,
      "grad_norm": 1.5081475973129272,
      "learning_rate": 5.9478260869565216e-05,
      "loss": 5.6403,
      "step": 7600
    },
    {
      "epoch": 4.463768115942029,
      "grad_norm": 1.3848239183425903,
      "learning_rate": 5.368115942028986e-05,
      "loss": 5.6375,
      "step": 7700
    },
    {
      "epoch": 4.521739130434782,
      "grad_norm": 1.2095786333084106,
      "learning_rate": 4.7884057971014495e-05,
      "loss": 5.6267,
      "step": 7800
    },
    {
      "epoch": 4.579710144927536,
      "grad_norm": 1.2248371839523315,
      "learning_rate": 4.208695652173913e-05,
      "loss": 5.6438,
      "step": 7900
    },
    {
      "epoch": 4.63768115942029,
      "grad_norm": 1.4772143363952637,
      "learning_rate": 3.628985507246377e-05,
      "loss": 5.6281,
      "step": 8000
    }
  ],
  "logging_steps": 100,
  "max_steps": 8625,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 38824574976000.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
