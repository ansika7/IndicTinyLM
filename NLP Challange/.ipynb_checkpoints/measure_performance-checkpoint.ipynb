{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53abc126-834d-4c6a-b52f-86a7eae42e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phenix Labs\\anaconda3\\envs\\mediapipe-env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Phenix Labs\\anaconda3\\envs\\mediapipe-env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import json\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sentencepiece as spm\n",
    "import os \n",
    "from datasets import Dataset\n",
    "import math\n",
    "\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02df6c3-1392-42a4-9eb4-c92a619ed383",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class PadCollator:\n",
    "    def __init__(self, pad_id=0, max_length=None):\n",
    "        self.pad_id = pad_id\n",
    "        self.max_length = max_length  # fixed max length for all batches\n",
    "\n",
    "    def __call__(self, features):\n",
    "        input_ids = [f[\"input_ids\"].clone().detach().long() for f in features]\n",
    "        labels = [f[\"labels\"].clone().detach().long() for f in features]\n",
    "\n",
    "        # Pad each sequence manually to fixed length\n",
    "        if self.max_length is not None:\n",
    "            input_ids = [self._pad_to_length(x, self.max_length, self.pad_id) for x in input_ids]\n",
    "            labels = [self._pad_to_length(x, self.max_length, -100) for x in labels]\n",
    "            input_ids = torch.stack(input_ids)\n",
    "            labels = torch.stack(labels)\n",
    "        else:\n",
    "            # Dynamic padding (default)\n",
    "            input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.pad_id)\n",
    "            labels = pad_sequence(labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "        return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "    def _pad_to_length(self, tensor, length, pad_value):\n",
    "        \"\"\"Pad or truncate a tensor to a fixed length.\"\"\"\n",
    "        if tensor.size(0) < length:\n",
    "            pad_size = length - tensor.size(0)\n",
    "            return torch.cat([tensor, torch.full((pad_size,), pad_value, dtype=tensor.dtype)])\n",
    "        else:\n",
    "            return tensor[:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "947f1c7c-0750-448c-b234-006521a7aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.inputs = df['input_ids'].tolist()\n",
    "        self.targets = df['target_ids'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.inputs[idx], dtype=torch.long),\n",
    "            'labels': torch.tensor(self.targets[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beaa5b6-76dc-4457-9b3c-30fc7243fdd8",
   "metadata": {},
   "source": [
    "## Loading validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fdf6d4a-ea24-4d50-98b8-9ac93fbc32ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval = pd.read_json(r'dataset/validation_mr.jsonl', lines=True)\n",
    "df_eval = pd.json_normalize(df_eval['row'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57200c07-06b1-4e76-b598-b5cf1bf75cb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>‡§î‡§∞‡§Ç‡§ó‡§æ‡§¨‡§æ‡§¶ : ‡§™‡•ç‡§∞‡§§‡§ø‡§®‡§ø‡§ß‡•Ä '‡§§‡•ç‡§Ø‡§æ‡§Ç‡§®‡•Ä ‡§Æ‡§≤‡§æ ‡§ñ‡•Ç‡§™ ‡§õ‡§≥‡§≤‡§Ç‡§Ø.</td>\n",
       "      <td>‡§Æ‡§≤‡§æ ‡§ñ‡•Ç‡§™  ‡§õ‡§≥‡§≤‡§Ç‡§Ø, ‡§§‡•ç‡§Ø‡§æ‡§Ç‡§®‡§æ‡§π‡•Ä ‡§õ‡§≥‡§æ</td>\n",
       "      <td>https://www.pudhari.news/news/Aurangabad/Polic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>‡§µ‡§ø‡§π‡§ø‡§∞‡•Ä‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§Ö‡§ú‡•ç‡§û‡§æ‡§§ ‡§á‡§∏‡§Æ‡§æ‡§ö‡§æ ‡§Æ‡•É‡§§‡§¶‡•á‡§π ‡§Ü‡§¢‡§≥‡§≤‡•ç‡§Ø‡§æ‡§®‡•á ‡§ñ‡§≥‡§¨...</td>\n",
       "      <td>‡§Æ‡•ã‡§¶‡•Ä‡§Ç‡§ö‡•Ä ‡§Æ‡•Å‡§ñ‡•ç‡§Ø‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä‡§™‡§¶‡§æ‡§ö‡•Ä ‡§ï‡§æ‡§∞‡§ï‡§ø‡§∞‡•ç‡§¶ ‡§¶‡•á‡§∂‡§æ‡§µ‡§∞‡§ö‡§æ ‡§°‡§æ‡§ó...</td>\n",
       "      <td>https://www.dainikprabhat.com/modis-chief-mini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>‡§Æ‡•Å‡§Ç‡§¨‡§à‡§É ‡§ï‡§æ‡§≤ ‡§∞‡§æ‡§§‡•ç‡§∞‡•Ä‡§™‡§æ‡§∏‡•Ç‡§® ‡§Æ‡•Å‡§Ç‡§¨‡§à‡§§ ‡§∏‡•Å‡§∞‡•Å ‡§Ö‡§∏‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§ú‡•ã...</td>\n",
       "      <td>‡§Æ‡§π‡§æ‡§™‡•å‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§®‡§æ‡§• ‡§Æ‡§π‡§æ‡§°‡•á‡§∂‡•ç‡§µ‡§∞‡§æ‡§Ç‡§Ç‡§®‡§Ç‡§§‡§∞ ‡§â‡§¶‡•ç‡§ß‡§µ ‡§†‡§æ‡§ï‡§∞‡•á‡§Ç‡§ö...</td>\n",
       "      <td>https://maharashtradesha.com/uddhav-thackerays...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>‡§ü‡•Ä‡§Æ ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§¶‡•á‡§∂‡§æ : ‡§∞‡§æ‡§ú‡•ç‡§Ø‡§æ‡§§ ‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ ‡§®‡§ø‡§µ‡§°‡§£‡•Å‡§ï...</td>\n",
       "      <td>‡§∂‡§∞‡§¶ ‡§™‡§µ‡§æ‡§∞‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ '‡§Ø‡§æ' ‡§≠‡•Ç‡§Æ‡§ø‡§ï‡•á‡§ö‡§æ ‡§∂‡§ø‡§µ‡§∏‡•á‡§®‡•á‡§≤‡§æ ‡§¨‡§∏‡§≤‡§æ ‡§¶‡§£‡§ï‡§æ</td>\n",
       "      <td>https://maharashtradesha.com/sharad-pawar-said...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>‡§™‡•Å‡§£‡•á : ‡§™‡•ç‡§∞‡§§‡§ø‡§®‡§ø‡§ß‡•Ä ‡§∂‡§æ‡§≤‡•á‡§Ø ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•Ä ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø...</td>\n",
       "      <td>‡§∂‡§æ‡§≥‡§æ, ‡§™‡§æ‡§≤‡§ï‡§æ‡§Ç‡§Æ‡•Å‡§≥‡•á ‡§ñ‡§æ‡§∏‡§ó‡•Ä ‡§µ‡§æ‡§π‡§§‡•Ç‡§ï ‡§´‡•ã‡§´‡§æ‡§µ‡§£‡§æ‡§∞</td>\n",
       "      <td>https://www.pudhari.news/news/Pune/Private-tra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                              input  \\\n",
       "0  1       ‡§î‡§∞‡§Ç‡§ó‡§æ‡§¨‡§æ‡§¶ : ‡§™‡•ç‡§∞‡§§‡§ø‡§®‡§ø‡§ß‡•Ä '‡§§‡•ç‡§Ø‡§æ‡§Ç‡§®‡•Ä ‡§Æ‡§≤‡§æ ‡§ñ‡•Ç‡§™ ‡§õ‡§≥‡§≤‡§Ç‡§Ø.   \n",
       "1  2  ‡§µ‡§ø‡§π‡§ø‡§∞‡•Ä‡§Æ‡§ß‡•ç‡§Ø‡•á ‡§Ö‡§ú‡•ç‡§û‡§æ‡§§ ‡§á‡§∏‡§Æ‡§æ‡§ö‡§æ ‡§Æ‡•É‡§§‡§¶‡•á‡§π ‡§Ü‡§¢‡§≥‡§≤‡•ç‡§Ø‡§æ‡§®‡•á ‡§ñ‡§≥‡§¨...   \n",
       "2  3  ‡§Æ‡•Å‡§Ç‡§¨‡§à‡§É ‡§ï‡§æ‡§≤ ‡§∞‡§æ‡§§‡•ç‡§∞‡•Ä‡§™‡§æ‡§∏‡•Ç‡§® ‡§Æ‡•Å‡§Ç‡§¨‡§à‡§§ ‡§∏‡•Å‡§∞‡•Å ‡§Ö‡§∏‡§≤‡•á‡§≤‡•ç‡§Ø‡§æ ‡§ú‡•ã...   \n",
       "3  4  ‡§ü‡•Ä‡§Æ ‡§Æ‡§π‡§æ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞ ‡§¶‡•á‡§∂‡§æ : ‡§∞‡§æ‡§ú‡•ç‡§Ø‡§æ‡§§ ‡§µ‡§ø‡§ß‡§æ‡§®‡§∏‡§≠‡§æ ‡§®‡§ø‡§µ‡§°‡§£‡•Å‡§ï...   \n",
       "4  5  ‡§™‡•Å‡§£‡•á : ‡§™‡•ç‡§∞‡§§‡§ø‡§®‡§ø‡§ß‡•Ä ‡§∂‡§æ‡§≤‡•á‡§Ø ‡§µ‡§ø‡§¶‡•ç‡§Ø‡§æ‡§∞‡•ç‡§•‡•ç‡§Ø‡§æ‡§Ç‡§ö‡•Ä ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø...   \n",
       "\n",
       "                                              target  \\\n",
       "0                      ‡§Æ‡§≤‡§æ ‡§ñ‡•Ç‡§™  ‡§õ‡§≥‡§≤‡§Ç‡§Ø, ‡§§‡•ç‡§Ø‡§æ‡§Ç‡§®‡§æ‡§π‡•Ä ‡§õ‡§≥‡§æ   \n",
       "1  ‡§Æ‡•ã‡§¶‡•Ä‡§Ç‡§ö‡•Ä ‡§Æ‡•Å‡§ñ‡•ç‡§Ø‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä‡§™‡§¶‡§æ‡§ö‡•Ä ‡§ï‡§æ‡§∞‡§ï‡§ø‡§∞‡•ç‡§¶ ‡§¶‡•á‡§∂‡§æ‡§µ‡§∞‡§ö‡§æ ‡§°‡§æ‡§ó...   \n",
       "2  ‡§Æ‡§π‡§æ‡§™‡•å‡§∞ ‡§µ‡§ø‡§∂‡•ç‡§µ‡§®‡§æ‡§• ‡§Æ‡§π‡§æ‡§°‡•á‡§∂‡•ç‡§µ‡§∞‡§æ‡§Ç‡§Ç‡§®‡§Ç‡§§‡§∞ ‡§â‡§¶‡•ç‡§ß‡§µ ‡§†‡§æ‡§ï‡§∞‡•á‡§Ç‡§ö...   \n",
       "3   ‡§∂‡§∞‡§¶ ‡§™‡§µ‡§æ‡§∞‡§æ‡§Ç‡§ö‡•ç‡§Ø‡§æ '‡§Ø‡§æ' ‡§≠‡•Ç‡§Æ‡§ø‡§ï‡•á‡§ö‡§æ ‡§∂‡§ø‡§µ‡§∏‡•á‡§®‡•á‡§≤‡§æ ‡§¨‡§∏‡§≤‡§æ ‡§¶‡§£‡§ï‡§æ   \n",
       "4             ‡§∂‡§æ‡§≥‡§æ, ‡§™‡§æ‡§≤‡§ï‡§æ‡§Ç‡§Æ‡•Å‡§≥‡•á ‡§ñ‡§æ‡§∏‡§ó‡•Ä ‡§µ‡§æ‡§π‡§§‡•Ç‡§ï ‡§´‡•ã‡§´‡§æ‡§µ‡§£‡§æ‡§∞   \n",
       "\n",
       "                                                 url  \n",
       "0  https://www.pudhari.news/news/Aurangabad/Polic...  \n",
       "1  https://www.dainikprabhat.com/modis-chief-mini...  \n",
       "2  https://maharashtradesha.com/uddhav-thackerays...  \n",
       "3  https://maharashtradesha.com/sharad-pawar-said...  \n",
       "4  https://www.pudhari.news/news/Pune/Private-tra...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2e62f7-bd2b-4457-868f-d0752149b82f",
   "metadata": {},
   "source": [
    "## Importing tokenizer and tokenizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c160ad16-e9ec-44c4-a3d6-7621e70cd76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('my_tokenizer.model')\n",
    "\n",
    "def encode(text):\n",
    "    return sp.encode(text, out_type=int)\n",
    "\n",
    "df_eval['input_ids'] = df_eval['input'].apply(encode)\n",
    "df_eval['target_ids'] = df_eval['target'].apply(encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b70f727-b04e-4022-91aa-0cd9e20decbf",
   "metadata": {},
   "source": [
    "## Loading GPT2 Model and testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64c739d5-2e77-41dd-ada6-cdb75b5b91f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='650' max='650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [650/650 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 8.093897819519043, 'eval_model_preparation_time': 0.001, 'eval_runtime': 16.5617, 'eval_samples_per_second': 156.988, 'eval_steps_per_second': 39.247}\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"./trained_model2\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=4,\n",
    ")\n",
    "#eval_dataset = SentencePieceEvalDataset(df_eval, sp, max_length=64)\n",
    "data_collator = PadCollator(pad_id=sp.pad_id(), max_length=64)\n",
    "dataset_eval = TextDataset(df_eval)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,  # your PadCollator\n",
    "    eval_dataset=dataset_eval,\n",
    ")\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17382a89-c55d-44e8-9877-cf20c24332b3",
   "metadata": {},
   "source": [
    "## Calculating perplexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7ca0c06-20d3-464b-8c9f-828879764eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 3274.43\n"
     ]
    }
   ],
   "source": [
    "perplexity = math.exp(results[\"eval_loss\"])\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8902692c-f34d-4697-8134-285b103340c5",
   "metadata": {},
   "source": [
    "## Cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63226cac-aac5-4b96-9983-ac57700eafd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy: 8.0939\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = results[\"eval_loss\"]\n",
    "print(f\"Cross Entropy: {cross_entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8885e2c2-0d4c-4d86-a1f2-f97c6cf62fcb",
   "metadata": {},
   "source": [
    "## Bits per token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18f1a1e8-5752-46d3-9511-4309e8aa3d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bits per Token: 11.6770\n"
     ]
    }
   ],
   "source": [
    "bpt = cross_entropy / math.log(2)\n",
    "print(f\"Bits per Token: {bpt:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a69ce4-b483-4895-b58a-1588d91137de",
   "metadata": {},
   "source": [
    "## Token-level Top-1 Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661531c-adbf-44a9-8b48-52f271b7f3bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def token_level_accuracy(model, dataloader, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            mask = labels != sp.pad_id()\n",
    "            correct += ((preds == labels) & mask).sum().item()\n",
    "            total += mask.sum().item()\n",
    "    return correct / total\n",
    "\n",
    "dataloader = DataLoader(dataset_eval, batch_size=4, collate_fn=data_collator)\n",
    "acc = token_level_accuracy(model, dataloader)\n",
    "print(f\"Top-1 Token Accuracy: {acc:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5af524a1-ca78-4cfb-850d-ec983e9f3cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2600 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m     tokens = sp.encode(example[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m], out_type=\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: tokens, \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m: tokens}\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m dataset_eval = \u001b[43mdataset_eval\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 1Ô∏è‚É£ CROSS ENTROPY\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[32m     37\u001b[39m training_args = TrainingArguments(\n\u001b[32m     38\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m     per_device_eval_batch_size=BATCH_SIZE,\n\u001b[32m     40\u001b[39m     report_to=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mediapipe-env\\Lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mediapipe-env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3341\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3339\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3340\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3341\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3342\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mediapipe-env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3673\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3671\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[32m   3672\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3673\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3674\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3675\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mediapipe-env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3647\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3645\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3646\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3647\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mediapipe-env\\Lib\\site-packages\\datasets\\arrow_dataset.py:3570\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3568\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3569\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3570\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3571\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mtokenize_function\u001b[39m\u001b[34m(example)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtokenize_function\u001b[39m(example):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     tokens = sp.encode(\u001b[43mexample\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, out_type=\u001b[38;5;28mint\u001b[39m)\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m: tokens, \u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m: tokens}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\mediapipe-env\\Lib\\site-packages\\datasets\\formatting\\formatting.py:283\u001b[39m, in \u001b[36mLazyDict.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    282\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.keys_to_format:\n\u001b[32m    285\u001b[39m         value = \u001b[38;5;28mself\u001b[39m.format(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'text'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import time\n",
    "import os, glob, psutil\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import GPT2LMHeadModel, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "# -------------------------------\n",
    "# CONFIGURATION\n",
    "# -------------------------------\n",
    "MODEL_PATH = \"./trained_model2\"\n",
    "MAX_LENGTH = 64\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on {DEVICE}\")\n",
    "\n",
    "# -------------------------------\n",
    "# LOAD MODEL\n",
    "# -------------------------------\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Suppose df_eval is your evaluation dataframe with a 'text' column\n",
    "# and `sp` is your SentencePiece tokenizer\n",
    "dataset_eval = Dataset.from_pandas(df_eval)\n",
    "\n",
    "def tokenize_function(example):\n",
    "    tokens = sp.encode(example[\"text\"], out_type=int)\n",
    "    return {\"input_ids\": tokens, \"labels\": tokens}\n",
    "\n",
    "dataset_eval = dataset_eval.map(tokenize_function)\n",
    "\n",
    "# -------------------------------\n",
    "# 1Ô∏è‚É£ CROSS ENTROPY\n",
    "# -------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    loss = eval_pred.loss\n",
    "    return {\"cross_entropy\": loss}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=dataset_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "results = trainer.evaluate()\n",
    "cross_entropy = results[\"eval_loss\"]\n",
    "print(f\"Cross Entropy: {cross_entropy:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2Ô∏è‚É£ BITS PER TOKEN\n",
    "# -------------------------------\n",
    "bpt = cross_entropy / math.log(2)\n",
    "print(f\"Bits per Token: {bpt:.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3Ô∏è‚É£ TOKEN-LEVEL TOP-1 ACCURACY\n",
    "# -------------------------------\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # simple padding for demonstration\n",
    "    max_len = min(MAX_LENGTH, max(len(x[\"input_ids\"]) for x in batch))\n",
    "    input_ids = [x[\"input_ids\"][:max_len] for x in batch]\n",
    "    labels = [x[\"labels\"][:max_len] for x in batch]\n",
    "    input_ids = torch.nn.utils.rnn.pad_sequence([torch.tensor(x) for x in input_ids],\n",
    "                                                batch_first=True, padding_value=sp.pad_id())\n",
    "    labels = torch.nn.utils.rnn.pad_sequence([torch.tensor(x) for x in labels],\n",
    "                                             batch_first=True, padding_value=sp.pad_id())\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "dataloader = DataLoader(dataset_eval, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "def token_level_accuracy(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids)\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            mask = labels != sp.pad_id()\n",
    "            correct += ((preds == labels) & mask).sum().item()\n",
    "            total += mask.sum().item()\n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "acc = token_level_accuracy(model, dataloader, DEVICE)\n",
    "print(f\"Top-1 Token Accuracy: {acc:.4%}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 4Ô∏è‚É£ PARAMETER COUNTS\n",
    "# -------------------------------\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total Parameters: {total_params:,}\")\n",
    "print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 5Ô∏è‚É£ INFERENCE LATENCY\n",
    "# -------------------------------\n",
    "sample_input = torch.tensor([sp.encode(\"Sample input text\", out_type=int)]).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    for _ in range(3): model(sample_input)  # warmup\n",
    "\n",
    "n_runs = 10\n",
    "start = time.time()\n",
    "for _ in range(n_runs):\n",
    "    with torch.no_grad():\n",
    "        model(sample_input)\n",
    "end = time.time()\n",
    "latency_ms = (end - start) / n_runs * 1000\n",
    "print(f\"Inference Latency: {latency_ms:.2f} ms per input\")\n",
    "\n",
    "# -------------------------------\n",
    "# 6Ô∏è‚É£ THROUGHPUT\n",
    "# -------------------------------\n",
    "tokens = sample_input.numel()\n",
    "throughput = tokens / (latency_ms / 1000)\n",
    "print(f\"Throughput: {throughput:.2f} tokens/s\")\n",
    "\n",
    "# -------------------------------\n",
    "# 7Ô∏è‚É£ FLOPs ESTIMATE\n",
    "# -------------------------------\n",
    "flops_per_token = 6 * total_params\n",
    "print(f\"Approx FLOPs per token: {flops_per_token / 1e9:.2f} GFLOPs\")\n",
    "\n",
    "# -------------------------------\n",
    "# 8Ô∏è‚É£ QUANTIZED MODEL SIZE (approx)\n",
    "# -------------------------------\n",
    "size_bytes = sum(os.path.getsize(f) for f in glob.glob(f\"{MODEL_PATH}/**/*\", recursive=True) if os.path.isfile(f))\n",
    "print(f\"Model disk size: {size_bytes / (1024 ** 2):.2f} MB\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9Ô∏è‚É£ PEAK CPU MEMORY USAGE\n",
    "# -------------------------------\n",
    "process = psutil.Process()\n",
    "cpu_before = process.memory_info().rss\n",
    "with torch.no_grad():\n",
    "    model(sample_input)\n",
    "cpu_after = process.memory_info().rss\n",
    "print(f\"Peak CPU memory usage (approx): {(cpu_after - cpu_before)/1e6:.2f} MB\")\n",
    "\n",
    "# -------------------------------\n",
    "# üîü SAMPLING EFFICIENCY CURVE\n",
    "# -------------------------------\n",
    "lengths = [8, 16, 32, 64, 128]\n",
    "efficiency = []\n",
    "\n",
    "for L in lengths:\n",
    "    inp = torch.tensor([sp.encode(\"test \" * L, out_type=int)]).to(DEVICE)\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        model(inp)\n",
    "    end = time.time()\n",
    "    tok_per_sec = L / (end - start)\n",
    "    efficiency.append(tok_per_sec)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(lengths, efficiency, marker='o')\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Tokens per Second\")\n",
    "plt.title(\"Sampling Efficiency Curve\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc06d8-d430-4f4c-bc91-dd1311f42712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5272bc8-dbe6-4ba8-9916-7211f445d265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
